# -*- coding: utf-8 -*-
"""Liver Patient.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O31oS0-R72nusf0m9mRa3dGSQjC8Zg9x
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import plotly.express as px
import matplotlib.pyplot as plt
from  google.colab import files
import io
import seaborn as sns
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, mean_absolute_error,r2_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,AdaBoostClassifier,BaggingClassifier,GradientBoostingClassifier
import xgboost as xgb
# %matplotlib inline

data=files.upload()

data=pd.read_csv('/content/indian_liver_patient.csv')

data.head()

data.info()

data.isnull().any()

data.isnull().sum()

data.Albumin_and_Globulin_Ratio.fillna(data['Albumin_and_Globulin_Ratio'].mean(),inplace=True)
data.isnull().sum()

data.head()

data.info()

data.describe()

from sklearn.preprocessing import LabelEncoder
lc = LabelEncoder()
data['Gender']= lc.fit_transform(data['Gender'])

data.describe()

sns.displot(data['Age'])
plt.title('Age Distribution Graph')
plt.show()

sns.countplot(x=data['Albumin'], hue=data['Gender'])

plt.figure(figsize=(10,7))
sns.heatmap(data.corr(),annot=True)

from sklearn.preprocessing import scale
x=data.iloc[:,:-1]
y=data['Dataset']
x_scaled=pd.DataFrame(scale(x), columns=x.columns)
x_scaled.head()

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)

pip install imblearn

from imblearn.over_sampling import SMOTE
smote = SMOTE()

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
x_train_std = sc.fit_transform(x_train)
x_test_std = sc.transform(x_test)

from imblearn.over_sampling import SMOTE
smote = SMOTE()
y_train.value_counts()

from sklearn.neighbors import KNeighborsClassifier

x_train_smote, y_train_smote=smote.fit_resample(x_train,y_train)
y_train_smote.value_counts()

from sklearn.ensemble import RandomForestClassifier
model1=RandomForestClassifier()
model1.fit(x_train_smote, y_train_smote)
y_predict=model1.predict(x_test)
rfc1=accuracy_score(y_test,y_predict)
rfc1
pd.crosstab(y_test, y_predict)
print(classification_report(y_test, y_predict))

from sklearn.tree import DecisionTreeClassifier
model4=DecisionTreeClassifier()
model4.fit(x_train_smote,y_train_smote)
y_predict=model4.predict(x_test)
dtc1=accuracy_score(y_test,y_predict)
dtc1
pd.crosstab(y_test,y_predict)
print(classification_report(y_test,y_predict))

from sklearn.neighbors import KNeighborsClassifier
model2=KNeighborsClassifier()
model2.fit(x_train_smote,y_train_smote)
y_predict=model2.predict(x_test)
knn1=(accuracy_score(y_test,y_predict))
knn1
pd.crosstab(y_test,y_predict)
print(classification_report(y_test,y_predict))

from sklearn.linear_model import LogisticRegression
model5=LogisticRegression()
model5.fit(x_train_smote,y_train_smote)
y_predict=model5.predict(x_test)
logi1=accuracy_score(y_test,y_predict)
logi1
pd.crosstab(y_test,y_test)
print(classification_report(y_test,y_predict))

import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Initialising the ANN
classifier = Sequential()
classifier.add(Dense(units=100,activation='relu',input_dim=10))
classifier.add(Dense(units=50,activation='relu'))
classifier.add(Dense(units=1,activation='sigmoid'))
classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

# Fitting the ANN to the Training set
model_history = classifier.fit(x_train, y_train, batch_size=100, validation_split=0.2, epochs=100)

model4.predict([[50,1,1.2,0.8,150,70,80,7.2,3.4,0.8]])

model1.predict([[50,1,1.2,0.8,150,70,80,7.2,3.4,0.8]])

classifier.save("liver.h5")
y_pred = classifier.predict(x_test)

y_pred
y_pred = (y_pred > 0.5)
y_pred

def predict_exit(sample_value):
  # Convert list to numpy array
  sample_value = np.array(sample_value)
  #Reshape because sample_value contains only 1 record
  sample_value = sample_value.reshape(1, -1)
  #Feature Scaling
  sample_value = scale(sample_value)
  return classifier.predict(sample_value)

#Age-->|Gender-->|Total_Bilrubin-->|Direct_Bilrubin->|Alkaline_phosphotase-->|
sample_value = [[50,1,1.2,0.8,150,70,80,7.2,3.4,0.8]]
if predict_exit(sample_value)>0.5:
  print('prediction: Liver Patient')
else:
    print('prediction: Healthy ')

acc_smote= [['KNN Classifier', knn1],  ['RandomForestClassifier', rfc1],
            ['DecisionTreeClassifier',dtc1],['LogisticRegression',logi1]]
Liverpatient_pred=pd.DataFrame(acc_smote, columns= ['classification models','accuracy_score'])
Liverpatient_pred

plt.figure(figsize=(7,5))
plt.xticks(rotation=90)
plt.title('Classification models & accuracy scores after SMOTE',fontsize=18)
sns.barplot(x="classification models",y="accuracy_score", data=Liverpatient_pred,palette ="Set2")

from sklearn.ensemble import ExtraTreesClassifier
model=ExtraTreesClassifier()
model.fit(x,y)

model.feature_importances_

dd=pd.DataFrame(model.feature_importances_,index=x.columns).sort_values(0,ascending=False)
dd

dd.plot(kind='barh',figsize=(7,6))
plt.title("FEATURE IMPORTANCE",fontsize=14)

import joblib
joblib.dump(model1, 'ETC.pkl')